Replit Agent Prompt — “Spark + Dask + XGBoost” upgrade
Branch: feature/spark–mllib-dask-switch
Goal: Replace Prophet / scikit-learn scripts with a Spark MLlib + Dask + XGBoost stack and expose new AI endpoints.

Task 1 — Bootstrap the new ML tool-chain
Files to edit / add


Action	Path	Notes
MODIFY	backend/Dockerfile	After the Node section, add:
RUN apt-get update && apt-get install -y python3 python3-pip
RUN pip3 install pyspark==3.5.1 "dask[complete]==2025.4.0" xgboost==2.0.3 shap==0.45.*
ADD	backend/requirements.ml.txt	List the four packages above (so future devs can pip install -r).
MODIFY	.github/workflows/ci.yml	In the backend test job, run pip install -r backend/requirements.ml.txt before npm test.
MODIFY	docs/ARCHITECTURE.md	Append a new “Big-Data ML” section summarising Spark MLlib + Dask + XGBoost + SHAP.
Acceptance criteria

docker build -f backend/Dockerfile . completes without errors.

CI green (unit tests still pass).

docs/ARCHITECTURE.md shows the new stack header.

Do not touch any other files.
Commit label: feat(ml): add Spark MLlib, Dask & XGBoost base image.

Task 2 — Create the Dask ETL skeleton
(Only attempt after Task 1 is merged)

ADD etl/dask_export.py

Connect to CockroachDB via psycopg2 (already in image).

Query journal_entries → write partitioned Parquet to data/raw/journal_entries/ using Dask.

Log row-count to stdout.

ADD dev script to package.json

json
Copy
Edit
"scripts": {
  "etl:je": "python3 etl/dask_export.py"
}
Test: new Jest file test/etl_dry_run.test.ts that shells npm run etl:je with CHILD_PROCESS.exec, asserts exit code 0.

Commit label: feat(etl): initial Dask → Parquet exporter.

Task 3 — Spark MLlib forecasting job
ADD ml/train_forecast_spark.py

Read the Parquet from Task 2.

For each entity_id, build an ARIMA(1,1,1) forecast (MLlib TimeSeriesModel).

Save model objects to models/forecast/{entity_id}.

ADD scripts/spark-submit.sh

bash
Copy
Edit
#!/usr/bin/env bash
spark-submit --master local[*] ml/train_forecast_spark.py
UPDATE CI to run the script in a separate job tagged forecast-test (just ensure it exits 0 on sample data).

Commit label: feat(ml): Spark MLlib ARIMA forecasting.

Task 4 — XGBoost anomaly model + SHAP explainability
ADD ml/train_anomaly_xgb.py – use dask_xgboost to train on JE features.

ADD ml/shap_explain.py – loads the XGB model, returns top-5 SHAP features as JSON.

MODIFY backend/routes/ai.js

GET /ai/forecasts/:entity → stream forecast JSON produced by Spark job.

GET /ai/anomalies/:entity → run python3 ml/shap_explain.py --entity <id> and return result.

Commit label: feat(api): AI endpoints (forecast, anomaly + SHAP).

Task 5 — Docs & cleanup
Update docs/ROADMAP.md task list to mark Tasks 1-4 complete.

Run npm run lint && npm run prettier and remove any unused imports (npx ts-prune).

Commit label: chore(docs): roadmap + lint after ML stack switch.

Definition of Done (for the whole upgrade)
Docker image ships with Spark 3.5, Dask 2025.4, XGBoost 2.0, SHAP 0.45.

npm run etl:je writes Parquet successfully.

scripts/spark-submit.sh trains and stores ARIMA models.

New /ai/forecasts/:entity and /ai/anomalies/:entity endpoints return 200 + JSON on sample data.

All unit tests and npm run verify pass in CI.

Agent, begin with Task 1 only.
Reply “Done ✅ Task 1” when the PR is open and CI is green.