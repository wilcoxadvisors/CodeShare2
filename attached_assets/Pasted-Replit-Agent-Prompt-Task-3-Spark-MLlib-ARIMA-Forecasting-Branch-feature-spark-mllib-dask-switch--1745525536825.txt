Replit-Agent Prompt — Task 3: Spark MLlib ARIMA Forecasting
Branch: feature/spark–mllib-dask-switch
Context: ✅ Task 2 merged. Parquet exporter + ETL CI job are live.
Goal: Train per-entity ARIMA models with Spark MLlib, stash artefacts, and add a “forecast” CI gate.

Task 3 — Build & test the Spark training pipeline

Action	Path	Details
ADD	ml/train_forecast_spark.py	Minimal MVP:
python<br>import argparse, pathlib, pyspark.sql as sql<br>from pyspark.ml.forecasting import ARIMA<br>spark = sql.SparkSession.builder.appName('forecast').getOrCreate()<br>df = spark.read.parquet('data/raw/journal_entries') # from Dask ETL<br>for eid in [r.entity_id for r in df.select('entity_id').distinct().collect()]:<br> series = df.filter(df.entity_id==eid).groupBy('date').sum('amount').orderBy('date')<br> model = ARIMA(p=1,d=1,q=1).fit(series)<br> out_dir = pathlib.Path('models/forecast')/str(eid); out_dir.mkdir(parents=True, exist_ok=True)<br> model.write().overwrite().save(str(out_dir))<br>print('✓ trained', eid)<br>
ADD	scripts/train_forecast.sh	bash<br>#!/usr/bin/env bash<br>spark-submit --master local[*] ml/train_forecast_spark.py "$@"<br> (chmod +x)
ADD	test/forecast_train.test.ts	Jest test shells bash scripts/train_forecast.sh --sample (you can add a --sample flag that limits Spark to one entity and 30 rows) and asserts exit 0.
MODIFY	.github/workflows/ci.yml	New job forecast-test after etl-test:
* checkout → install Python deps → run scripts/train_forecast.sh --sample.
* Use same Docker image (no Spark cluster needed; local[*] works).
MODIFY	docs/ARCHITECTURE.md	Extend Big-Data ML section with “ARIMA model artefacts stored under models/forecast/{entity_id}”.
Acceptance Criteria
Running bash scripts/train_forecast.sh --sample locally finishes ≤60 s and prints “✓ trained”.

models/forecast/<id>/ folders appear with Spark metadata files (model dir or part-*.parquet).

New Jest test passes (npm test).

CI shows four green jobs: backend-test, frontend-test (if any), etl-test, forecast-test.

No eslint/prettier violations.

Commit label: feat(ml): Spark ARIMA training & CI job

Guard-rails
Diff ≤ 200 LoC; touch only listed files.

Spark runs in local mode; no cluster provisioning.

Keep existing shell wrappers (scripts/…) pattern—don’t alter package.json.

Run npm run verify before opening PR.

Reply exactly “Done ✅ Task 3” once the PR is open and CI is green.

Agent, begin Task 3 now.